<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../../jacoco-resources/report.gif" type="image/gif"/><title>RemoteMonitoringService.java</title><link rel="stylesheet" href="../../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../../index.html" class="el_report">debian</a> &gt; <a href="../index.html" class="el_bundle">infrastructure</a> &gt; <a href="index.source.html" class="el_package">org.corfudb.infrastructure</a> &gt; <span class="el_source">RemoteMonitoringService.java</span></div><h1>RemoteMonitoringService.java</h1><pre class="source lang-java linenums">package org.corfudb.infrastructure;

import com.google.common.collect.ImmutableSet;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import lombok.AllArgsConstructor;
import lombok.Getter;
import lombok.NonNull;
import lombok.Setter;
import lombok.extern.slf4j.Slf4j;
import org.corfudb.common.result.Result;
import org.corfudb.infrastructure.management.ClusterAdvisor;
import org.corfudb.infrastructure.management.ClusterAdvisorFactory;
import org.corfudb.infrastructure.management.ClusterStateContext;
import org.corfudb.infrastructure.management.ClusterType;
import org.corfudb.infrastructure.management.FailureDetector;
import org.corfudb.infrastructure.management.PollReport;
import org.corfudb.infrastructure.management.ReconfigurationEventHandler;
import org.corfudb.infrastructure.management.failuredetector.ClusterGraph;
import org.corfudb.protocols.wireprotocol.ClusterState;
import org.corfudb.protocols.wireprotocol.NodeState;
import org.corfudb.protocols.wireprotocol.SequencerMetrics;
import org.corfudb.protocols.wireprotocol.failuredetector.FailureDetectorMetrics;
import org.corfudb.protocols.wireprotocol.failuredetector.FailureDetectorMetrics.FailureDetectorAction;
import org.corfudb.protocols.wireprotocol.failuredetector.NodeRank;
import org.corfudb.runtime.CorfuRuntime;
import org.corfudb.runtime.exceptions.QuorumUnreachableException;
import org.corfudb.runtime.exceptions.unrecoverable.UnrecoverableCorfuInterruptedError;
import org.corfudb.runtime.view.Layout;
import org.corfudb.util.LambdaUtils;
import org.corfudb.util.concurrent.SingletonResource;

import java.time.Duration;
import java.util.Collections;
import java.util.HashSet;
import java.util.Map;
import java.util.Optional;
import java.util.Set;
import java.util.TreeSet;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicLong;
import java.util.function.Function;
import java.util.function.Supplier;
import java.util.stream.Collectors;

/**
 * Remote Monitoring Service constitutes of failure and healing monitoring and handling.
 * This service is responsible for heartbeat and aggregating the cluster view. This is
 * updated in the shared context with the management server which serves the heartbeat responses.
 * The failure detector updates the unreachable nodes in the layout.
 * The healing detector heals nodes which were previously marked unresponsive but have now healed.
 * Created by zlokhandwala on 11/2/18.
 */
<span class="fc" id="L58">@Slf4j</span>
public class RemoteMonitoringService implements MonitoringService {

<span class="fc" id="L61">    private static final CompletableFuture&lt;DetectorTask&gt; DETECTOR_TASK_COMPLETED</span>
<span class="fc" id="L62">            = CompletableFuture.completedFuture(DetectorTask.COMPLETED);</span>

<span class="fc" id="L64">    private static final CompletableFuture&lt;DetectorTask&gt; DETECTOR_TASK_NOT_COMPLETED</span>
<span class="fc" id="L65">            = CompletableFuture.completedFuture(DetectorTask.NOT_COMPLETED);</span>

<span class="fc" id="L67">    private static final CompletableFuture&lt;DetectorTask&gt; DETECTOR_TASK_SKIPPED</span>
<span class="fc" id="L68">            = CompletableFuture.completedFuture(DetectorTask.SKIPPED);</span>

    /**
     * Detectors to be used to detect failures and healing.
     */
<span class="fc" id="L73">    @Getter</span>
    private final FailureDetector failureDetector;

    /**
     * Detection Task Scheduler Service
     * This service schedules the following tasks every POLICY_EXECUTE_INTERVAL (1 sec):
     * - Detection of failed nodes.
     * - Detection of healed nodes.
     */
<span class="nc" id="L82">    @Getter</span>
    private final ScheduledExecutorService detectionTasksScheduler;
    /**
     * To dispatch tasks for failure or healed nodes detection.
     */
<span class="fc" id="L87">    @Getter</span>
    private final ExecutorService failureDetectorWorker;

    private final ServerContext serverContext;
    private final SingletonResource&lt;CorfuRuntime&gt; runtimeSingletonResource;
    private final ClusterStateContext clusterContext;
    private final LocalMonitoringService localMonitoringService;

    /**
     * Future for periodic failure and healed nodes detection task.
     */
<span class="fc" id="L98">    private CompletableFuture&lt;DetectorTask&gt; failureDetectorFuture = DETECTOR_TASK_NOT_COMPLETED;</span>

    private final ClusterAdvisor advisor;

    /**
     * The management agent attempts to bootstrap a NOT_READY sequencer if the
     * sequencerNotReadyCounter counter exceeds this value.
     */
    private static final int SEQUENCER_NOT_READY_THRESHOLD = 3;

    /**
     * Failure detector counter. Keeps count of missing runs of failure detector
     * if the detector is busy from previous run
     */
<span class="fc" id="L112">    private final AtomicLong counter = new AtomicLong(1);</span>

    /**
     * Number of workers for failure detector. Three workers used by default:
     * - failure/healing detection
     * - bootstrap sequencer
     * - merge segments
     */
<span class="fc" id="L120">    private final int detectionWorkersCount = 3;</span>

    /**
     * Future which is reset every time a new task to mergeSegments is launched.
     * This is to avoid multiple mergeSegments requests.
     */
<span class="fc" id="L126">    private volatile CompletableFuture&lt;Boolean&gt; mergeSegmentsTask = CompletableFuture.completedFuture(true);</span>

    /**
     * Duration in which the restore redundancy and merge segments workflow status is queried.
     */
<span class="fc" id="L131">    private static final Duration MERGE_SEGMENTS_RETRY_QUERY_TIMEOUT = Duration.ofSeconds(1);</span>

    /**
     * This tuple maintains, in an epoch, how many heartbeats the primary sequencer has responded
     * in not bootstrapped (NOT_READY) state.
     */
    @Getter
<span class="nc" id="L138">    @Setter</span>
<span class="fc" id="L139">    @AllArgsConstructor</span>
    private class SequencerNotReadyCounter {
<span class="fc" id="L141">        private final long epoch;</span>
<span class="fc" id="L142">        private int counter;</span>

        public void increment() {
<span class="fc" id="L145">            counter += 1;</span>
<span class="fc" id="L146">        }</span>
    }

<span class="fc" id="L149">    private volatile SequencerNotReadyCounter sequencerNotReadyCounter = new SequencerNotReadyCounter(0, 0);</span>

<span class="pc bpc" id="L151" title="1 of 2 branches missed.">    RemoteMonitoringService(@NonNull ServerContext serverContext,</span>
<span class="pc bpc" id="L152" title="1 of 2 branches missed.">                            @NonNull SingletonResource&lt;CorfuRuntime&gt; runtimeSingletonResource,</span>
<span class="pc bpc" id="L153" title="1 of 2 branches missed.">                            @NonNull ClusterStateContext clusterContext,</span>
<span class="pc bpc" id="L154" title="1 of 2 branches missed.">                            @NonNull FailureDetector failureDetector,</span>
<span class="pc bpc" id="L155" title="1 of 2 branches missed.">                            @NonNull LocalMonitoringService localMonitoringService) {</span>
<span class="fc" id="L156">        this.serverContext = serverContext;</span>
<span class="fc" id="L157">        this.runtimeSingletonResource = runtimeSingletonResource;</span>
<span class="fc" id="L158">        this.clusterContext = clusterContext;</span>
<span class="fc" id="L159">        this.failureDetector = failureDetector;</span>
<span class="fc" id="L160">        this.localMonitoringService = localMonitoringService;</span>
<span class="fc" id="L161">        this.advisor = ClusterAdvisorFactory.createForStrategy(</span>
                ClusterType.COMPLETE_GRAPH,
<span class="fc" id="L163">                serverContext.getLocalEndpoint()</span>
        );

<span class="fc" id="L166">        this.detectionTasksScheduler = Executors.newSingleThreadScheduledExecutor(</span>
                new ThreadFactoryBuilder()
<span class="fc" id="L168">                        .setDaemon(true)</span>
<span class="fc" id="L169">                        .setNameFormat(serverContext.getThreadPrefix() + &quot;ManagementService&quot;)</span>
<span class="fc" id="L170">                        .build());</span>

        // Creating the detection worker thread pool.
        // This thread pool is utilized to dispatch detection tasks at regular intervals in the
        // detectorTaskScheduler.
<span class="fc" id="L175">        this.failureDetectorWorker = Executors.newFixedThreadPool(</span>
                detectionWorkersCount,
                new ThreadFactoryBuilder()
<span class="fc" id="L178">                        .setDaemon(true)</span>
<span class="fc" id="L179">                        .setNameFormat(serverContext.getThreadPrefix() + &quot;DetectionWorker-%d&quot;)</span>
<span class="fc" id="L180">                        .build()</span>
        );
<span class="fc" id="L182">    }</span>

    private CorfuRuntime getCorfuRuntime() {
<span class="fc" id="L185">        return runtimeSingletonResource.get();</span>
    }

    /**
     * Executes task to run failure and healing detection every poll interval. (Default: 1 sec)
     * &lt;p&gt;
     * During the initialization step, the method:
     * - triggers sequencer bootstrap
     * - starts failure and healing detection mechanism, by running &lt;code&gt;runDetectionTasks&lt;/code&gt;
     * every second (by default). Next iteration of detection task can be run only if current iteration is completed.
     */
    @Override
    public void start(Duration monitoringInterval) {
        // Trigger sequencer bootstrap on startup.
<span class="fc" id="L199">        sequencerBootstrap(serverContext);</span>

<span class="fc" id="L201">        detectionTasksScheduler.scheduleAtFixedRate(</span>
<span class="fc" id="L202">                () -&gt; LambdaUtils.runSansThrow(this::runDetectionTasks),</span>
                0,
<span class="fc" id="L204">                monitoringInterval.toMillis(),</span>
                TimeUnit.MILLISECONDS
        );
<span class="fc" id="L207">    }</span>

    /**
     * Trigger sequencer bootstrap mechanism. Get current layout management view and execute async bootstrap
     *
     * @param serverContext server context
     */
    private CompletableFuture&lt;DetectorTask&gt; sequencerBootstrap(ServerContext serverContext) {
<span class="fc" id="L215">        log.info(&quot;Trigger sequencer bootstrap on startup&quot;);</span>
<span class="fc" id="L216">        return getCorfuRuntime()</span>
<span class="fc" id="L217">                .getLayoutManagementView()</span>
<span class="fc" id="L218">                .asyncSequencerBootstrap(serverContext.copyManagementLayout(), failureDetectorWorker)</span>
<span class="fc" id="L219">                .thenApply(DetectorTask::fromBool);</span>
    }

    /**
     * Checks if this management client is allowed to handle reconfigurations.
     * - This client is not authorized to trigger reconfigurations if this node is not a part
     * of the current layout.
     *
     * @return True if node is allowed to handle reconfigurations. False otherwise.
     */
    private boolean canHandleReconfigurations() {

        // We check for the following condition here: If the node is NOT a part of the
        // current layout, it should not attempt to change layout.
<span class="fc" id="L233">        Layout layout = serverContext.getManagementLayout();</span>
<span class="pc bpc" id="L234" title="1 of 2 branches missed.">        if (!layout.getAllServers().contains(serverContext.getLocalEndpoint())) {</span>
<span class="nc" id="L235">            log.debug(&quot;This Server is not a part of the active layout. Aborting reconfiguration handling.&quot;);</span>
<span class="nc" id="L236">            return false;</span>
        }
<span class="fc" id="L238">        return true;</span>
    }


    /**
     * Schedules the failure detection and handling mechanism by detectorTaskScheduler.
     * It schedules exactly one instance of the following tasks.
     * - Failure detection tasks.
     * - Healing detection tasks.
     *
     * &lt;pre&gt;
     * The algorithm:
     *  - wait until previous iteration finishes
     *  - On every invocation, this task refreshes the runtime to fetch the latest layout and also updates
     *  the local persisted copy of the latest layout.
     *  - get corfu metrics (Sequencer, LogUnit etc).
     *  - executes the poll using the failureDetector which generates a pollReport at the end of the round.
     *  The report contains latest information about cluster: connectivity graph, failed and healed nodes, wrong epochs.
     *  - refresh cluster state context by latest {@link ClusterState} collected on poll report step.
     *  - run failure detector task composed from:
     *     - the outOfPhase epoch server errors are corrected by resealing and patching these trailing layout servers.
     *     - all unresponsive server failures are handled by either removing or marking
     *     them as unresponsive based on a failure handling policy.
     *    - make healed node responsive based on a healing detection mechanism
     *  &lt;/pre&gt;
     */
    private synchronized void runDetectionTasks() {

<span class="fc" id="L266">        Layout ourLayout = getCorfuRuntime()</span>
<span class="fc" id="L267">                .invalidateLayout()</span>
<span class="fc" id="L268">                .thenApply(serverContext::saveManagementLayout)</span>
<span class="fc" id="L269">                .join();</span>

<span class="pc bpc" id="L271" title="1 of 2 branches missed.">        if (!canHandleReconfigurations()) {</span>
<span class="nc" id="L272">            log.error(&quot;Can't run failure detector. This Server: {}, is not a part of the active layout: {}&quot;,</span>
<span class="nc" id="L273">                    serverContext.getLocalEndpoint(), ourLayout);</span>
<span class="nc" id="L274">            return;</span>
        }

<span class="fc bfc" id="L277" title="All 2 branches covered.">        if (!failureDetectorFuture.isDone()) {</span>
<span class="fc" id="L278">            log.trace(&quot;Cannot initiate new failure detection task. Polling in progress. Counter: {}&quot;, counter.get());</span>
<span class="fc" id="L279">            counter.incrementAndGet();</span>
<span class="fc" id="L280">            return;</span>
        }

<span class="fc" id="L283">        counter.set(1);</span>

<span class="fc" id="L285">        failureDetectorFuture =</span>
                //Get metrics from local monitoring service (local monitoring works in it's own thread)
<span class="fc" id="L287">                localMonitoringService.getMetrics()</span>
                        //Poll report asynchronously using failureDetectorWorker executor
<span class="fc" id="L289">                        .thenCompose(metrics -&gt; pollReport(ourLayout, metrics))</span>
                        //Update cluster view by latest cluster state given by the poll report. No need to be asynchronous
<span class="fc" id="L291">                        .thenApply(pollReport -&gt; {</span>
<span class="fc" id="L292">                            log.trace(&quot;Update cluster view: {}&quot;, pollReport.getClusterState());</span>
<span class="fc" id="L293">                            clusterContext.refreshClusterView(ourLayout, pollReport);</span>
<span class="fc" id="L294">                            return pollReport;</span>
                        })
                        //Execute failure detector task using failureDetectorWorker executor
<span class="fc" id="L297">                        .thenCompose(pollReport -&gt; runFailureDetectorTask(pollReport, ourLayout))</span>
                        //Print exceptions to log
<span class="fc" id="L299">                        .whenComplete((taskResult, ex) -&gt; {</span>
<span class="fc bfc" id="L300" title="All 2 branches covered.">                            if (ex != null) {</span>
<span class="fc" id="L301">                                log.error(&quot;Failure detection task finished with error&quot;, ex);</span>
                            }
<span class="fc" id="L303">                        });</span>
<span class="fc" id="L304">    }</span>

    private CompletableFuture&lt;PollReport&gt; pollReport(Layout layout, SequencerMetrics sequencerMetrics) {
<span class="fc" id="L307">        return CompletableFuture.supplyAsync(() -&gt; {</span>
<span class="fc" id="L308">            CorfuRuntime corfuRuntime = getCorfuRuntime();</span>

<span class="fc" id="L310">            return failureDetector.poll(layout, corfuRuntime, sequencerMetrics);</span>
        }, failureDetectorWorker);
    }

    /**
     * &lt;pre&gt;
     * Failure/Healing detector task. Detects faults in the cluster and heals servers also corrects wrong epochs:
     *  - correct wrong epochs by resealing the servers and updating trailing layout servers.
     *    Fetch quorum layout and update the epoch according to the quorum.
     *  - check if current layout slot is unfilled then update layout to latest one.
     *  - handle healed nodes.
     *  - Looking for link failures in the cluster, handle failure if found.
     *  - Restore redundancy and merge segments if present in the layout.
     *  - bootstrap sequencer if needed
     * &lt;/pre&gt;
     *
     * @param pollReport cluster status
     * @return async detection task
     */
    private CompletableFuture&lt;DetectorTask&gt; runFailureDetectorTask(
            PollReport pollReport, Layout ourLayout) {

<span class="fc bfc" id="L332" title="All 2 branches covered.">        if (!pollReport.getClusterState().isReady()) {</span>
<span class="fc" id="L333">            log.info(&quot;Cluster state is not ready: {}&quot;, pollReport.getClusterState());</span>
<span class="fc" id="L334">            return DETECTOR_TASK_SKIPPED;</span>
        }

<span class="fc" id="L337">        return CompletableFuture.supplyAsync(() -&gt; {</span>

            // Corrects out of phase epoch issues if present in the report. This method
            // performs re-sealing of all nodes if required and catchup of a layout server to
            // the current state.
<span class="fc" id="L342">            final Layout latestLayout = correctWrongEpochs(pollReport, ourLayout);</span>

<span class="fc" id="L344">            Result&lt;DetectorTask, RuntimeException&gt; failure = Result.of(() -&gt; {</span>

                // This is just an optimization in case we receive a WrongEpochException
                // while one of the other management clients is trying to move to a new layout.
                // This check is merely trying to minimize the scenario in which we end up
                // filling the slot with an outdated layout.
<span class="fc bfc" id="L350" title="All 2 branches covered.">                if (!pollReport.areAllResponsiveServersSealed()) {</span>
<span class="fc" id="L351">                    log.debug(&quot;All responsive servers have not been sealed yet. Skipping.&quot;);</span>
<span class="fc" id="L352">                    return DetectorTask.COMPLETED;</span>
                }

<span class="fc" id="L355">                Optional&lt;Long&gt; unfilledSlot = pollReport.getLayoutSlotUnFilled(latestLayout);</span>
                // If the latest slot has not been filled, fill it with the previous known layout.
<span class="fc bfc" id="L357" title="All 2 branches covered.">                if (unfilledSlot.isPresent()) {</span>
<span class="fc" id="L358">                    log.info(&quot;Trying to fill an unfilled slot {}. PollReport: {}&quot;,</span>
<span class="fc" id="L359">                            unfilledSlot.get(), pollReport);</span>
<span class="fc" id="L360">                    detectFailure(latestLayout, Collections.emptySet(), pollReport).join();</span>
<span class="fc" id="L361">                    return DetectorTask.COMPLETED;</span>
                }

<span class="fc bfc" id="L364" title="All 2 branches covered.">                if (!pollReport.getWrongEpochs().isEmpty()) {</span>
<span class="fc" id="L365">                    log.debug(&quot;Wait for next iteration. Poll report contains wrong epochs: {}&quot;,</span>
<span class="fc" id="L366">                            pollReport.getWrongEpochs()</span>
                    );
<span class="fc" id="L368">                    return DetectorTask.COMPLETED;</span>
                }

                // If layout was updated by correcting wrong epochs,
                // we can't continue with failure detection,
                // as the cluster state have changed.
<span class="pc bpc" id="L374" title="1 of 2 branches missed.">                if (!latestLayout.equals(ourLayout)){</span>
<span class="nc" id="L375">                    log.warn(&quot;Layout was updated by correcting wrong epochs. &quot; +</span>
                            &quot;Cancel current round of failure detection.&quot;);
<span class="nc" id="L377">                    return DetectorTask.COMPLETED;</span>
                }

<span class="fc" id="L380">                return DetectorTask.NOT_COMPLETED;</span>
            });

<span class="pc" id="L383">            failure.ifError(err -&gt; log.error(&quot;Can't fill slot. Poll report: {}&quot;, pollReport, err));</span>

<span class="pc bpc" id="L385" title="1 of 4 branches missed.">            if (failure.isValue() &amp;&amp; failure.get() == DetectorTask.COMPLETED) {</span>
<span class="fc" id="L386">                return DetectorTask.COMPLETED;</span>
            }

<span class="fc" id="L389">            DetectorTask healing = detectHealing(pollReport, ourLayout);</span>

            //If local node healed it causes change in the cluster state which means the layout is changed also.
            //If the cluster status is changed let failure detector detect the change on next iteration and
            //behave according to latest cluster state.
<span class="pc bpc" id="L394" title="1 of 2 branches missed.">            if (healing == DetectorTask.COMPLETED) {</span>
<span class="nc" id="L395">                return DetectorTask.COMPLETED;</span>
            }

            // Analyze the poll report and trigger failure handler if needed.
<span class="fc" id="L399">            DetectorTask handleFailure = detectFailure(pollReport, ourLayout);</span>

            //If a failure is detected (which means we have updated a layout)
            // then don't try to heal anything, wait for next iteration.
<span class="fc bfc" id="L403" title="All 2 branches covered.">            if (handleFailure == DetectorTask.COMPLETED) {</span>
<span class="fc" id="L404">                return DetectorTask.COMPLETED;</span>
            }

            // Restores redundancy and merges multiple segments if present.
<span class="fc" id="L408">            restoreRedundancyAndMergeSegments(ourLayout);</span>

<span class="fc" id="L410">            handleSequencer(ourLayout);</span>

<span class="fc" id="L412">            return DetectorTask.COMPLETED;</span>
        }, failureDetectorWorker);
    }

    /**
     * Spawns a new asynchronous task to restore redundancy and merge segments.
     * A new task is not spawned if a task is already in progress.
     * This method does not wait on the completion of the restore redundancy and merge segments task.
     *
     * @return Detector task.
     */
    private DetectorTask restoreRedundancyAndMergeSegments(Layout layout) {
<span class="fc" id="L424">        int segmentsCount = layout.getSegments().size();</span>

<span class="fc bfc" id="L426" title="All 2 branches covered.">        if (segmentsCount == 1) {</span>
<span class="fc" id="L427">            log.debug(&quot;No segments to merge. Skipping step.&quot;);</span>
<span class="fc" id="L428">            return DetectorTask.SKIPPED;</span>
<span class="pc bpc" id="L429" title="1 of 2 branches missed.">        } else if (!mergeSegmentsTask.isDone()) {</span>
<span class="nc" id="L430">            log.debug(&quot;Merge segments task already in progress. Skipping spawning another task.&quot;);</span>
<span class="nc" id="L431">            return DetectorTask.SKIPPED;</span>
        }

<span class="fc" id="L434">        log.debug(&quot;Number of segments present: {}. Spawning task to merge segments.&quot;, segmentsCount);</span>

<span class="fc" id="L436">        Supplier&lt;Boolean&gt; redundancyAction = () -&gt;</span>
<span class="fc" id="L437">                ReconfigurationEventHandler.handleMergeSegments(</span>
<span class="fc" id="L438">                        getCorfuRuntime(), layout, MERGE_SEGMENTS_RETRY_QUERY_TIMEOUT</span>
                );
<span class="fc" id="L440">        mergeSegmentsTask = CompletableFuture.supplyAsync(redundancyAction, failureDetectorWorker);</span>

<span class="fc" id="L442">        return DetectorTask.COMPLETED;</span>
    }

    /**
     * Handle healed node.
     * Cluster advisor provides healed node based on current cluster state if healed node found then
     * save healed node in the history and send a message with the detected healed node
     * to the relevant management server.
     *
     * @param pollReport poll report
     * @param layout     current layout
     */
    private DetectorTask detectHealing(PollReport pollReport, Layout layout) {
<span class="fc" id="L455">        log.trace(&quot;Handle healing, layout: {}&quot;, layout);</span>

<span class="fc" id="L457">        Optional&lt;NodeRank&gt; healed = advisor.healedServer(pollReport.getClusterState());</span>

        //Transform Optional value to a Set
<span class="fc" id="L460">        Set&lt;String&gt; healedNodes = healed</span>
<span class="fc" id="L461">                .map(NodeRank::getEndpoint)</span>
<span class="fc" id="L462">                .map(ImmutableSet::of)</span>
<span class="fc" id="L463">                .orElse(ImmutableSet.of());</span>

<span class="fc bfc" id="L465" title="All 2 branches covered.">        if (healedNodes.isEmpty()) {</span>
<span class="fc" id="L466">            log.trace(&quot;Nothing to heal&quot;);</span>
<span class="fc" id="L467">            return DetectorTask.SKIPPED;</span>
        }

        //save history
<span class="fc" id="L471">        FailureDetectorMetrics history = FailureDetectorMetrics.builder()</span>
<span class="fc" id="L472">                .localNode(serverContext.getLocalEndpoint())</span>
<span class="fc" id="L473">                .graph(advisor.getGraph(pollReport.getClusterState()).connectivityGraph())</span>
<span class="fc" id="L474">                .healed(healed.get())</span>
<span class="fc" id="L475">                .action(FailureDetectorAction.HEAL)</span>
<span class="fc" id="L476">                .unresponsiveNodes(layout.getUnresponsiveServers())</span>
<span class="fc" id="L477">                .layout(layout.getLayoutServers())</span>
<span class="fc" id="L478">                .epoch(layout.getEpoch())</span>
<span class="fc" id="L479">                .build();</span>

<span class="fc" id="L481">        log.info(&quot;Handle healing. Failure detector state: {}&quot;, history.toJson());</span>

        try {
<span class="fc" id="L484">            CorfuRuntime corfuRuntime = getCorfuRuntime();</span>

<span class="fc" id="L486">            corfuRuntime.getLayoutView()</span>
<span class="fc" id="L487">                    .getRuntimeLayout(layout)</span>
<span class="fc" id="L488">                    .getManagementClient(serverContext.getLocalEndpoint())</span>
                    //handle healing asynchronously
<span class="nc" id="L490">                    .handleHealing(pollReport.getPollEpoch(), healedNodes)</span>
                    //completable future: wait this future to complete and get result
<span class="nc" id="L492">                    .get();</span>

<span class="nc" id="L494">            serverContext.saveFailureDetectorMetrics(history);</span>

<span class="nc" id="L496">            log.info(&quot;Healing local node successful: {}&quot;, history.toJson());</span>

<span class="nc" id="L498">            return DetectorTask.COMPLETED;</span>
<span class="nc" id="L499">        } catch (ExecutionException ee) {</span>
<span class="nc" id="L500">            log.error(&quot;Healing local node failed: &quot;, ee);</span>
<span class="nc" id="L501">        } catch (InterruptedException ie) {</span>
<span class="nc" id="L502">            log.error(&quot;Healing local node interrupted: &quot;, ie);</span>
<span class="nc" id="L503">            throw new UnrecoverableCorfuInterruptedError(ie);</span>
<span class="nc" id="L504">        }</span>

<span class="nc" id="L506">        return DetectorTask.NOT_COMPLETED;</span>
    }

    /**
     * Analyzes the poll report and triggers the failure handler if node failure detected.
     * ClusterAdvisor provides a failed node in the cluster.
     * If a failed node have found:
     * - save detected failure in the history
     * - handle failure
     *
     * @param pollReport Poll report obtained from failure detection policy.
     * @return boolean result if failure was handled. False if there is no failure
     */
    private DetectorTask detectFailure(PollReport pollReport, Layout layout) {
<span class="fc" id="L520">        log.trace(&quot;Handle failures for the report: {}&quot;, pollReport);</span>

        try {
<span class="fc" id="L523">            ClusterState clusterState = pollReport.getClusterState();</span>

<span class="pc bpc" id="L525" title="1 of 2 branches missed.">            if (clusterState.size() != layout.getAllServers().size()) {</span>
<span class="nc" id="L526">                String err = String.format(</span>
                        &quot;Cluster representation is different than layout. Cluster: %s, layout: %s&quot;,
                        clusterState, layout
                );
<span class="nc" id="L530">                throw new IllegalStateException(err);</span>
            }

<span class="fc" id="L533">            Optional&lt;NodeRank&gt; maybeFailedNode = advisor.failedServer(clusterState);</span>

<span class="fc bfc" id="L535" title="All 2 branches covered.">            if (maybeFailedNode.isPresent()) {</span>
<span class="fc" id="L536">                NodeRank failedNode = maybeFailedNode.get();</span>

                //Collect failures history
<span class="fc" id="L539">                FailureDetectorMetrics history = FailureDetectorMetrics.builder()</span>
<span class="fc" id="L540">                        .localNode(serverContext.getLocalEndpoint())</span>
<span class="fc" id="L541">                        .graph(advisor.getGraph(pollReport.getClusterState()).connectivityGraph())</span>
<span class="fc" id="L542">                        .healed(failedNode)</span>
<span class="fc" id="L543">                        .action(FailureDetectorAction.FAIL)</span>
<span class="fc" id="L544">                        .unresponsiveNodes(layout.getUnresponsiveServers())</span>
<span class="fc" id="L545">                        .layout(layout.getLayoutServers())</span>
<span class="fc" id="L546">                        .epoch(layout.getEpoch())</span>
<span class="fc" id="L547">                        .build();</span>

<span class="fc" id="L549">                serverContext.saveFailureDetectorMetrics(history);</span>

<span class="fc" id="L551">                Set&lt;String&gt; failedNodes = new HashSet&lt;&gt;();</span>
<span class="fc" id="L552">                failedNodes.add(failedNode.getEndpoint());</span>
<span class="fc" id="L553">                return detectFailure(layout, failedNodes, pollReport).get();</span>
            }
<span class="fc" id="L555">        } catch (Exception e) {</span>
<span class="fc" id="L556">            log.error(&quot;Exception invoking failure handler&quot;, e);</span>
<span class="fc" id="L557">        }</span>

<span class="fc" id="L559">        return DetectorTask.NOT_COMPLETED;</span>
    }

    /**
     * Checks sequencer state, triggers a new task to bootstrap the sequencer for the specified layout (if needed).
     *
     * @param layout current layout
     */
    private CompletableFuture&lt;DetectorTask&gt; handleSequencer(Layout layout) {
<span class="fc" id="L568">        log.trace(&quot;Handling sequencer failures&quot;);</span>

<span class="fc" id="L570">        ClusterState clusterState = clusterContext.getClusterView();</span>
<span class="fc" id="L571">        Optional&lt;NodeState&gt; primarySequencer = clusterState.getNode(layout.getPrimarySequencer());</span>
<span class="pc bpc" id="L572" title="2 of 4 branches missed.">        if (primarySequencer.isPresent() &amp;&amp; primarySequencer.get().getSequencerMetrics() == SequencerMetrics.READY) {</span>
<span class="nc" id="L573">            log.trace(&quot;Primary sequencer is already ready at: {} in {}&quot;, primarySequencer.get(), clusterState);</span>
<span class="nc" id="L574">            return DETECTOR_TASK_SKIPPED;</span>
        }

        // If failures are not present we can check if the primary sequencer has been
        // bootstrapped from the heartbeat responses received.
<span class="fc bfc" id="L579" title="All 2 branches covered.">        if (sequencerNotReadyCounter.getEpoch() != layout.getEpoch()) {</span>
            // If the epoch is different than the poll epoch, we reset the timeout state.
<span class="fc" id="L581">            log.trace(&quot;Current epoch is different to layout epoch. Update current epoch to: {}&quot;, layout.getEpoch());</span>
<span class="fc" id="L582">            sequencerNotReadyCounter = new SequencerNotReadyCounter(layout.getEpoch(), 1);</span>
<span class="fc" id="L583">            return DETECTOR_TASK_SKIPPED;</span>
        }

        // If the epoch is same as the epoch being tracked in the tuple, we need to
        // increment the count and attempt to bootstrap the sequencer if the count has
        // crossed the threshold.
<span class="fc" id="L589">        sequencerNotReadyCounter.increment();</span>
<span class="fc bfc" id="L590" title="All 2 branches covered.">        if (sequencerNotReadyCounter.getCounter() &lt; SEQUENCER_NOT_READY_THRESHOLD) {</span>
<span class="fc" id="L591">            return DETECTOR_TASK_SKIPPED;</span>
        }

        // Launch task to bootstrap the primary sequencer.
<span class="fc" id="L595">        log.info(&quot;Attempting to bootstrap the primary sequencer. ClusterState {}&quot;, clusterState);</span>
        // We do not care about the result of the trigger.
        // If it fails, we detect this again and retry in the next polling cycle.
<span class="fc" id="L598">        return getCorfuRuntime()</span>
<span class="fc" id="L599">                .getLayoutManagementView()</span>
<span class="fc" id="L600">                .asyncSequencerBootstrap(layout, failureDetectorWorker)</span>
<span class="fc" id="L601">                .thenApply(DetectorTask::fromBool);</span>
    }

    /**
     * Handle failures, sending message with detected failure to relevant management server.
     *
     * @param failedNodes list of failed nodes
     * @param pollReport  poll report
     */
    private CompletableFuture&lt;DetectorTask&gt; detectFailure(
            Layout layout, Set&lt;String&gt; failedNodes, PollReport pollReport) {

<span class="fc" id="L613">        ClusterGraph graph = advisor.getGraph(pollReport.getClusterState());</span>

<span class="fc" id="L615">        log.info(&quot;Detected failed nodes in node responsiveness: Failed:{}, is slot unfilled: {}, clusterState:{}&quot;,</span>
<span class="fc" id="L616">                failedNodes, pollReport.getLayoutSlotUnFilled(layout), graph.toJson()</span>
        );

<span class="fc" id="L619">        return getCorfuRuntime()</span>
<span class="fc" id="L620">                .getLayoutView()</span>
<span class="fc" id="L621">                .getRuntimeLayout(layout)</span>
<span class="fc" id="L622">                .getManagementClient(serverContext.getLocalEndpoint())</span>
<span class="fc" id="L623">                .handleFailure(layout.getEpoch(), failedNodes)</span>
<span class="fc" id="L624">                .thenApply(DetectorTask::fromBool);</span>
    }

    /**
     * Get the layout from a particular layout server requested by a Layout request message stamped
     * with the epoch from the specified layout.
     *
     * @param layout   Layout epoch to stamp the layout request.
     * @param endpoint Layout Server endpoint to request the layout from.
     * @return Completable future which returns the result of the RPC request.
     */
    private CompletableFuture&lt;Layout&gt; getLayoutFromServer(Layout layout, String endpoint) {
<span class="fc" id="L636">        CompletableFuture&lt;Layout&gt; completableFuture = new CompletableFuture&lt;&gt;();</span>
        try {
<span class="fc" id="L638">            completableFuture = getCorfuRuntime()</span>
<span class="fc" id="L639">                    .getLayoutView()</span>
<span class="fc" id="L640">                    .getRuntimeLayout(layout)</span>
<span class="fc" id="L641">                    .getLayoutClient(endpoint)</span>
<span class="fc" id="L642">                    .getLayout();</span>
<span class="nc" id="L643">        } catch (Exception e) {</span>
<span class="nc" id="L644">            completableFuture.completeExceptionally(e);</span>
<span class="fc" id="L645">        }</span>
<span class="fc" id="L646">        return completableFuture;</span>
    }

    /**
     * Corrects out of phase epochs by resealing the servers.
     * This would also need to update trailing layout servers.
     *
     * @param pollReport Poll Report from running the failure detection policy.
     */
    private Layout correctWrongEpochs(PollReport pollReport, Layout layout) {

<span class="fc" id="L657">        Map&lt;String, Long&gt; wrongEpochs = pollReport.getWrongEpochs();</span>
<span class="fc bfc" id="L658" title="All 2 branches covered.">        if (wrongEpochs.isEmpty()) {</span>
<span class="fc" id="L659">            return layout;</span>
        }

<span class="fc" id="L662">        log.debug(&quot;Correct wrong epochs. Poll report: {}&quot;, pollReport);</span>

        try {
<span class="fc" id="L665">            final Layout oldLayout = layout;</span>
            // Query all layout servers to get quorum Layout.
<span class="fc" id="L667">            Map&lt;String, CompletableFuture&lt;Layout&gt;&gt; layoutCompletableFutureMap = layout</span>
<span class="fc" id="L668">                    .getLayoutServers()</span>
<span class="fc" id="L669">                    .stream()</span>
<span class="fc" id="L670">                    .collect(Collectors.toMap(Function.identity(),</span>
<span class="fc" id="L671">                            server -&gt; getLayoutFromServer(oldLayout, server))</span>
                    );

            // Retrieve the correct layout from quorum of members to reseal servers.
            // If we are unable to reach a consensus from a quorum we get an exception and
            // abort the epoch correction phase.
<span class="fc" id="L677">            Optional&lt;Layout&gt; latestLayout = fetchLatestLayout(layoutCompletableFutureMap);</span>

<span class="pc bpc" id="L679" title="1 of 2 branches missed.">            if (!latestLayout.isPresent()) {</span>
<span class="nc" id="L680">                log.error(&quot;Can't get a layout from any server in the cluster. Layout servers: {}, wrong epochs: {}&quot;,</span>
<span class="nc" id="L681">                        layout.getLayoutServers(), wrongEpochs</span>
                );
<span class="nc" id="L683">                throw new IllegalStateException(&quot;Error in correcting server epochs. Local node is disconnected&quot;);</span>
            }

            // Update local layout copy.
<span class="fc" id="L687">            Layout newManagementLayout = serverContext.saveManagementLayout(latestLayout.get());</span>

<span class="fc" id="L689">            sealWithLatestLayout(pollReport, newManagementLayout);</span>

            // Check if any layout server has a stale layout.
            // If yes patch it (commit) with the latestLayout.
<span class="fc" id="L693">            updateTrailingLayoutServers(newManagementLayout, layoutCompletableFutureMap);</span>
<span class="fc" id="L694">            return newManagementLayout;</span>

<span class="fc" id="L696">        } catch (QuorumUnreachableException e) {</span>
<span class="fc" id="L697">            log.error(&quot;Error in correcting server epochs&quot;, e);</span>
        }

<span class="fc" id="L700">        return serverContext.copyManagementLayout();</span>
    }

    /**
     * This function will attempt to seal the cluster with the epoch provided
     * by the layout parameter.
     *
     * @param pollReport immutable poll report
     * @param managementLayout mutable layout that will not be modified
     */
    private void sealWithLatestLayout(PollReport pollReport, Layout managementLayout) {
        // We should utilize only the unmodified management layout as it has already been
        // committed to the layout servers via Paxos round.
        // Committing any other modified layout is extremely dangerous and can cause
        // inconsistencies. This latestLayout should not be modified.
<span class="fc" id="L715">        Layout sealingLayout = new Layout(managementLayout);</span>

        // In case of a partial seal, a set of servers can be sealed with a higher epoch.
        // We should be able to detect this and bring the rest of the servers to this epoch.
<span class="fc" id="L719">        pollReport.getLayoutSlotUnFilled(sealingLayout).ifPresent(sealingLayout::setEpoch);</span>

        // Re-seal all servers with the latestLayout epoch.
        // This has no effect on up-to-date servers. Only the trailing servers are caught up.
<span class="fc" id="L723">        getCorfuRuntime()</span>
<span class="fc" id="L724">                .getLayoutView()</span>
<span class="fc" id="L725">                .getRuntimeLayout(sealingLayout)</span>
<span class="fc" id="L726">                .sealMinServerSet();</span>
<span class="fc" id="L727">    }</span>

    /**
     * Fetches the latest layout from the cluster.
     *
     * @return quorum agreed layout.
     * @throws QuorumUnreachableException If unable to receive consensus on layout.
     */
    private Optional&lt;Layout&gt; fetchLatestLayout(Map&lt;String, CompletableFuture&lt;Layout&gt;&gt; futureLayouts) {
        //Sort layouts according to epochs
<span class="fc" id="L737">        TreeSet&lt;Layout&gt; layouts = new TreeSet&lt;&gt;(Layout.LAYOUT_COMPARATOR);</span>

<span class="fc" id="L739">        futureLayouts.values()</span>
<span class="fc" id="L740">                .stream()</span>
                //transform exceptions (connection errors) to optional values
<span class="fc" id="L742">                .map(async -&gt; async.handle((layout, ex) -&gt; {</span>
                    //Ignore all connection errors
<span class="fc bfc" id="L744" title="All 2 branches covered.">                    if (ex != null) {</span>
<span class="fc" id="L745">                        return Optional.&lt;Layout&gt;empty();</span>
                    }

<span class="fc" id="L748">                    return Optional.of(layout);</span>
                }))
                //Get results synchronously
<span class="fc" id="L751">                .map(CompletableFuture::join)</span>
                //Add all layouts to the set
<span class="fc" id="L753">                .forEach(optionalLayout -&gt; optionalLayout.ifPresent(layouts::add));</span>

<span class="fc" id="L755">        return Optional.ofNullable(layouts.first());</span>
    }

    /**
     * Finds all trailing layout servers and patches them with the latest persisted layout
     * retrieved by quorum.
     *
     * @param layoutCompletableFutureMap Map of layout server endpoints to their layout requests.
     */
    private void updateTrailingLayoutServers(
            Layout latestLayout, Map&lt;String, CompletableFuture&lt;Layout&gt;&gt; layoutCompletableFutureMap) {

        // Patch trailing layout servers with latestLayout.
<span class="fc" id="L768">        layoutCompletableFutureMap.keySet().forEach(layoutServer -&gt; {</span>
<span class="fc" id="L769">            Layout layout = null;</span>
            try {
<span class="fc" id="L771">                layout = layoutCompletableFutureMap.get(layoutServer).get();</span>
<span class="fc" id="L772">            } catch (ExecutionException ee) {</span>
                // Expected wrong epoch exception if layout server fell behind and has stale
                // layout and server epoch.
<span class="fc" id="L775">                log.warn(&quot;updateTrailingLayoutServers: layout fetch from {} failed: {}&quot;,</span>
                        layoutServer, ee);
<span class="nc" id="L777">            } catch (InterruptedException ie) {</span>
<span class="nc" id="L778">                log.error(&quot;updateTrailingLayoutServers: layout fetch from {} failed: {}&quot;,</span>
                        layoutServer, ie);
<span class="nc" id="L780">                throw new UnrecoverableCorfuInterruptedError(ie);</span>
<span class="fc" id="L781">            }</span>

            // Do nothing if this layout server is updated with the latestLayout.
<span class="fc bfc" id="L784" title="All 4 branches covered.">            if (layout != null &amp;&amp; layout.equals(latestLayout)) {</span>
<span class="fc" id="L785">                return;</span>
            }
            try {
                // Committing this layout directly to the trailing layout servers.
                // This is safe because this layout is acquired by a quorum fetch which confirms
                // that there was a consensus on this layout and has been committed to a quorum.
<span class="fc" id="L791">                boolean result = getCorfuRuntime()</span>
<span class="fc" id="L792">                        .getLayoutView()</span>
<span class="fc" id="L793">                        .getRuntimeLayout(latestLayout)</span>
<span class="fc" id="L794">                        .getLayoutClient(layoutServer)</span>
<span class="fc" id="L795">                        .committed(latestLayout.getEpoch(), latestLayout)</span>
<span class="fc" id="L796">                        .get();</span>
<span class="pc bpc" id="L797" title="1 of 2 branches missed.">                if (result) {</span>
<span class="fc" id="L798">                    log.debug(&quot;Layout Server: {} successfully patched with latest layout : {}&quot;,</span>
                            layoutServer, latestLayout);
                } else {
<span class="nc" id="L801">                    log.debug(&quot;Layout Server: {} patch with latest layout failed : {}&quot;, layoutServer, latestLayout);</span>
                }
<span class="fc" id="L803">            } catch (ExecutionException ee) {</span>
<span class="fc" id="L804">                log.error(&quot;Updating layout servers failed due to&quot;, ee);</span>
<span class="nc" id="L805">            } catch (InterruptedException ie) {</span>
<span class="nc" id="L806">                log.error(&quot;Updating layout servers failed due to&quot;, ie);</span>
<span class="nc" id="L807">                throw new UnrecoverableCorfuInterruptedError(ie);</span>
<span class="fc" id="L808">            }</span>
<span class="fc" id="L809">        });</span>
<span class="fc" id="L810">    }</span>

    @Override
    public void shutdown() {
        // Shutting the fault detector.
<span class="fc" id="L815">        detectionTasksScheduler.shutdownNow();</span>
<span class="fc" id="L816">        failureDetectorWorker.shutdownNow();</span>
<span class="fc" id="L817">        log.info(&quot;Fault Detection MonitoringService shutting down.&quot;);</span>
<span class="fc" id="L818">    }</span>

<span class="pc" id="L820">    public enum DetectorTask {</span>
        /**
         * The task is completed successfully
         */
<span class="fc" id="L824">        COMPLETED,</span>
        /**
         * The task is completed with an exception
         */
<span class="fc" id="L828">        NOT_COMPLETED,</span>
        /**
         * Skipped task
         */
<span class="fc" id="L832">        SKIPPED;</span>

        public static DetectorTask fromBool(boolean taskResult) {
<span class="fc bfc" id="L835" title="All 2 branches covered.">            return taskResult ? COMPLETED : NOT_COMPLETED;</span>
        }
    }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.9.201702052155</span></div></body></html>